# ChatMind - AI Memory Pipeline

A modular pipeline that transforms ChatGPT exports into a semantic knowledge graph with interactive visualization.

## ğŸ§­ Overview

This project ingests ChatGPT exports, processes them through semantic embedding and clustering, loads relationships into Neo4j, and provides a mindmap-style frontend for exploration.

## ğŸ—ï¸ Architecture

```
ChatGPT Exports â†’ Data Ingestion â†’ Embedding & Clustering â†’ Neo4j Graph â†’ Interactive Frontend
```

## ğŸ“ Project Structure

```
chatmind/
â”œâ”€â”€ data_ingestion/          # Extract and flatten chat data
â”‚   â”œâ”€â”€ extract_and_flatten.py
â”‚   â”œâ”€â”€ data_lake_storage.py # Data lake storage system
â”‚   â””â”€â”€ chatgpt_url_mapper.py # ChatGPT URL mapping
â”œâ”€â”€ embedding/               # Semantic embedding and clustering
â”‚   â””â”€â”€ embed_and_cluster.py
            â”œâ”€â”€ semantic_chunker/        # GPT-driven semantic chunking
            â”‚   â”œâ”€â”€ chunker.py          # Main chunking logic
            â”‚   â”œâ”€â”€ prompts.py          # Prompt templates
            â”‚   â”œâ”€â”€ run_chunking.py     # Script entry point
            â”‚   â””â”€â”€ test_sample.json    # Sample for testing
            â”œâ”€â”€ tagger/                  # GPT-driven auto-tagging
            â”‚   â”œâ”€â”€ tagger.py           # Main tagging logic
            â”‚   â”œâ”€â”€ prompts.py          # Tagging prompt templates
            â”‚   â”œâ”€â”€ run_tagging.py      # Script entry point
            â”‚   â””â”€â”€ test_chunks.jsonl   # Sample chunks for testing
            â”œâ”€â”€ cost_tracker/            # OpenAI API cost tracking
            â”‚   â””â”€â”€ tracker.py          # Cost tracking logic
            â”œâ”€â”€ neo4j_loader/           # Load data into Neo4j graph
            â”‚   â””â”€â”€ load_graph.py
            â”œâ”€â”€ api/                    # FastAPI backend
            â”‚   â””â”€â”€ main.py
            â”œâ”€â”€ frontend/               # React + Cytoscape visualization
            â”‚   â”œâ”€â”€ src/
            â”‚   â”‚   â”œâ”€â”€ App.tsx        # Main visualization component
            â”‚   â”‚   â”œâ”€â”€ components/    # React components
            â”‚   â”‚   â”‚   â”œâ”€â”€ TagFilter.tsx    # Tag filtering component
            â”‚   â”‚   â”‚   â””â”€â”€ CostTracker.tsx  # Cost tracking component
            â”‚   â”‚   â””â”€â”€ index.tsx      # React entry point
            â”‚   â””â”€â”€ package.json
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ raw/               # ChatGPT export ZIP files
â”‚   â”œâ”€â”€ processed/         # Flattened JSONL data
â”‚   â”œâ”€â”€ embeddings/        # Embeddings and clusters
â”‚   â””â”€â”€ lake/              # Data lake storage
â”‚       â”œâ”€â”€ chats/         # Individual chat JSON files
â”‚       â”œâ”€â”€ messages/      # Individual message JSON files
â”‚       â”œâ”€â”€ urls/          # ChatGPT URL mappings
â”‚       â””â”€â”€ metadata/      # Indexes and metadata
â”œâ”€â”€ run_pipeline.py        # Complete pipeline runner
â”œâ”€â”€ start_services.py      # Service starter
â””â”€â”€ requirements.txt       # Python dependencies
```

## ğŸš€ Quick Start

1. **Setup Environment**
   ```bash
   # Install Python dependencies
   pip install -r requirements.txt
   
   # Copy and configure environment variables
   cp env.example .env
   # Edit .env with your Neo4j and OpenAI credentials
   ```

2. **Setup Neo4j** (if running locally)
   ```bash
   # Install Neo4j Desktop or use Docker
   docker run -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/password neo4j:latest
   ```

3. **Process Data**
   ```bash
   # Place ChatGPT export ZIP files in data/raw/
   python run_pipeline.py
   ```

4. **Start Services**
   ```bash
   # Start API and frontend together
   python start_services.py
   
   # Or start individually:
   # API only: python chatmind/api/main.py
   # Frontend only: cd chatmind/frontend && npm start
   ```

5. **Access the Application**
   - Frontend: http://localhost:3000
   - API Docs: http://localhost:8000/docs

## ğŸ”§ Configuration

Copy `env.example` to `.env` and configure:

### Required Environment Variables
- `NEO4J_URI` - Neo4j connection string (default: bolt://localhost:7687)
- `NEO4J_USER` - Neo4j username (default: neo4j)
- `NEO4J_PASSWORD` - Neo4j password (default: password)

### Optional Environment Variables
- `OPENAI_API_KEY` - For enhanced embeddings/summaries (optional)
- `DEBUG` - Enable debug logging (default: True)
- `LOG_LEVEL` - Logging level (default: INFO)

## ğŸ“Š Data Flow

1. **Ingestion**: ZIP files â†’ content-based deduplication â†’ JSONL
2. **Chunking**: Conversations â†’ semantic chunks (GPT-driven or simple)
3. **Embedding**: Text chunks â†’ semantic vectors
4. **Clustering**: Vectors â†’ topic clusters
5. **Graph**: Clusters â†’ Neo4j knowledge graph
6. **Visualization**: Graph â†’ Interactive mindmap

## ğŸ”„ Re-ingestion & Deduplication

ChatMind handles overlapping ChatGPT exports intelligently:

- **Content-Based Deduplication**: Uses SHA256 hashes of normalized chat content
- **Smart Normalization**: Excludes timestamps and metadata that change between exports
- **Persistent State**: Remembers all processed content between runs
- **Overlap Handling**: Processes all ZIP files but only adds unique conversations

**Example Workflow:**
```bash
# First export (50 conversations)
cp chat_export_1.zip data/raw/
python run_pipeline.py
# Result: 50 new conversations added

# Second export (same 50 + 10 new conversations)
cp chat_export_2.zip data/raw/
python run_pipeline.py
# Result: Only 10 new conversations added, 50 duplicates skipped
```

## ğŸŠ Data Lake Architecture

ChatMind implements a hierarchical data lake for deep navigation:

### **ğŸ“ Data Lake Structure**
```
data/lake/
â”œâ”€â”€ chats/           # Individual chat JSON files (chat_<hash>.json)
â”œâ”€â”€ messages/        # Individual message JSON files (msg_<hash>.json)
â”œâ”€â”€ urls/            # ChatGPT URL mappings (conversation_id.json)
â””â”€â”€ metadata/        # Indexes and metadata
```

### **ğŸ”— Navigation Flow**
```
Neo4j Graph â†’ Chat Node â†’ Data Lake Chat â†’ Individual Messages
```

### **ğŸ¯ API Endpoints**
- `GET /lake/chat/{chat_id}` - Get specific chat details
- `GET /lake/chat/{chat_id}/messages` - Get all messages for chat
- `GET /lake/message/{message_id}` - Get specific message details
- `GET /lake/chat/{chat_id}/urls` - Get ChatGPT URLs for chat
- `GET /lake/url/{conversation_id}` - Get URL mapping by conversation ID
- `GET /lake/urls/search?query={title}` - Search URLs by chat title

### **ğŸ’¡ Benefits**
- **Hierarchical Navigation**: Drill down from graph â†’ chat â†’ message
- **Fast Access**: Direct file-based retrieval for individual items
- **Full Context**: Complete conversation history preserved
- **Scalable**: Efficient storage for large datasets
- **Direct ChatGPT Links**: One-click access to original conversations

## ğŸ§  Semantic Chunking

ChatMind supports both simple and GPT-driven semantic chunking:

### **ğŸ” Chunking Methods**
- **Simple Chunking**: Sentence-based splitting (default)
- **Semantic Chunking**: GPT-driven intelligent splitting
- **Fallback Safety**: Automatic fallback if GPT fails

### **ğŸ¯ Semantic Chunking Benefits**
- **Better Coherence**: Chunks focus on specific topics/themes
- **Context Preservation**: Maintains conversation flow
- **Meaningful Titles**: GPT-generated descriptive titles
- **Improved Clustering**: Better semantic grouping
- **Flexible Prompts**: Multiple styles (standard, detailed, technical)

### **ğŸ’¡ Usage Examples**
```bash
# Use semantic chunking in pipeline
python chatmind/embedding/embed_and_cluster.py --semantic-chunking

# Standalone semantic chunking
python chatmind/semantic_chunker/run_chunking.py

# Test with sample data
python demo_semantic_chunking.py
```

### **ğŸ”„ Prompt Styles**
- **Standard**: General conversation chunking
- **Detailed**: Complex technical discussions
- **Technical**: Code-heavy conversations

## ğŸ·ï¸ Auto-Tagging

ChatMind supports automatic tagging of semantic chunks using GPT:

### **ğŸ” Tagging Features**
- **Automatic Hashtags**: GPT-generated relevant hashtags
- **Semantic Categories**: Intelligent categorization
- **Multiple Styles**: Standard, detailed, and general prompts
- **Fallback Safety**: Automatic fallback if GPT fails
- **Rich Metadata**: Tagging model, timestamps, statistics

### **ğŸ¯ Tagging Benefits**
- **Better Organization**: Automatic content categorization
- **Improved Search**: Hashtag-based filtering and discovery
- **Semantic Understanding**: GPT-driven content analysis
- **Flexible Prompts**: Multiple tagging styles for different content
- **Statistics**: Tag frequency analysis and insights

### **ğŸ’¡ Usage Examples**
```bash
# Use auto-tagging in pipeline
python chatmind/embedding/embed_and_cluster.py --auto-tagging

# Standalone auto-tagging
python chatmind/tagger/run_tagging.py

# Test with sample data
python demo_auto_tagging.py
```

### **ğŸ”„ Tagging Styles**
- **Standard**: General content tagging
- **Detailed**: Technical content with comprehensive tags
- **General**: Non-technical conversation tagging

            ### **ğŸ“Š Output Format**
            ```json
            {
              "chat_id": "abc123",
              "chunk_id": "abc123_semantic_0",
              "title": "Introduction to FastAPI",
              "content": "USER: I want to build a web API...",
              "tags": ["#python", "#fastapi", "#web-api", "#tutorial"],
              "category": "Web API Development",
              "tagging_model": "gpt-4",
              "tagging_timestamp": 1703123456
            }
            ```

            ## ğŸ·ï¸ Tag-Based Filtering

            ChatMind supports powerful tag-based filtering for discovering and exploring content:

            ### **ğŸ” Filtering Features**
            - **Multi-Tag Selection**: Filter by multiple hashtags simultaneously
            - **Category Filtering**: Filter by semantic categories
            - **Search Integration**: Combine tags with text search
            - **Real-Time Updates**: Graph updates instantly when filters change
            - **Active Filter Display**: Visual indicators of current filters
            - **Clear Filters**: Easy reset to show full graph

            ### **ğŸ¯ Filtering Benefits**
            - **Content Discovery**: Find related conversations quickly
            - **Topic Focus**: Zoom in on specific areas of interest
            - **Cross-Reference**: See connections between different topics
            - **Efficient Navigation**: Skip to relevant content directly
            - **Visual Clarity**: Reduced graph complexity with focused views

            ### **ğŸ’¡ Usage Examples**
            ```bash
            # Filter by single tag
            Select "#python" to see all Python-related content

            # Filter by multiple tags
            Select "#python" AND "#api" to see Python API discussions

            # Filter by category
            Select "Web Development" category

            # Combined filtering
            Filter by "#python" category AND search for "FastAPI"
            ```

            ### **ğŸ”„ API Endpoints**
            - `GET /tags` - Get all available tags and categories
            - `GET /search/by-tags` - Search messages by tags/category
            - `GET /graph/filtered` - Get filtered graph data

            ### **ğŸ¨ Frontend Component**
            - **TagFilter.tsx**: React component for tag selection
            - **Multi-select dropdown**: Choose multiple tags
            - **Category dropdown**: Filter by semantic categories
            - **Search integration**: Combine with text search
            - **Active filters display**: Visual filter indicators
            - **Clear filters button**: Reset to full graph

            ## ğŸ’° Cost Tracking

            ChatMind includes comprehensive cost tracking for OpenAI API usage:

            ### **ğŸ“Š Cost Tracking Features**
            - **Real-Time Monitoring**: Track all API calls automatically
            - **Cost Calculation**: Automatic pricing based on OpenAI rates
            - **Usage Statistics**: Success rates, token usage, model breakdowns
            - **Daily Trends**: Cost trends over time
            - **Export Functionality**: JSON export for analysis
            - **API Endpoints**: REST API for frontend integration

            ### **ğŸ¯ Cost Tracking Benefits**
            - **Budget Management**: Monitor and control API costs
            - **Usage Analytics**: Understand usage patterns
            - **Performance Tracking**: Monitor success/failure rates
            - **Cost Optimization**: Identify expensive operations
            - **Usage Insights**: Token usage and model performance

            ### **ğŸ’¡ Usage Examples**
            ```bash
            # View cost statistics
            GET /costs/statistics

            # Get recent API calls
            GET /costs/recent?limit=20

            # View daily costs
            GET /costs/daily?days=30

            # Export cost data
            POST /costs/export
            ```

            ### **ğŸ¨ Frontend Dashboard**
            - **CostTracker.tsx**: React component for cost display
            - **Cost overview**: Total cost, calls, success rate
            - **Model breakdown**: Cost by model (GPT-4, GPT-3.5, etc.)
            - **Operation analysis**: Cost by operation (chunking, tagging)
            - **Recent activity**: Latest API calls with status
            - **Daily trends**: Cost trends over time
            - **Filtering**: Date ranges and operation filters

## ğŸ”— ChatGPT URL Mapping

ChatMind automatically extracts and maps ChatGPT conversation URLs:

### **ğŸ” URL Extraction**
- **Automatic Detection**: Scans chat content for ChatGPT URLs
- **Pattern Matching**: Supports multiple URL formats:
  - `https://chat.openai.com/c/{conversation_id}`
  - `https://chat.openai.com/share/{conversation_id}`
  - `https://chat.openai.com/chat/{conversation_id}`
- **Persistent Storage**: URLs stored in data lake for fast access

### **ğŸ¯ Navigation Flow**
```
Neo4j Graph â†’ Chat Node â†’ Get URLs â†’ Open in ChatGPT
```

### **ğŸ’¡ Usage Examples**
```bash
# Get URLs for a specific chat
curl http://localhost:8000/lake/chat/chat_abc123/urls

# Search URLs by title
curl http://localhost:8000/lake/urls/search?query=python

# Get URL mapping details
curl http://localhost:8000/lake/url/conversation_id_123
```

### **ğŸ”„ Benefits**
- **Direct Access**: One-click links to original ChatGPT conversations
- **Search Integration**: Find conversations by title or content
- **Batch Export**: Get all URLs for a specific chat
- **Frontend Integration**: Easy to add "Open in ChatGPT" buttons

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.8+**: Core language
- **FastAPI**: Modern, fast web framework
- **Neo4j**: Graph database for knowledge storage
- **sentence-transformers**: Local embedding generation
- **HDBSCAN**: Density-based clustering
- **UMAP**: Dimensionality reduction

### Frontend
- **React 18**: Modern UI framework
- **TypeScript**: Type-safe development
- **Material-UI**: Beautiful, accessible components
- **Cytoscape.js**: Interactive graph visualization
- **Axios**: HTTP client for API communication

### Data Processing
- **JSONL**: Efficient data storage format
- **Embeddings**: 384-dimensional semantic vectors
- **Graph Relationships**: Topic â†’ Chat â†’ Message hierarchy

## ğŸ“ Project Structure

```
chatmind/
â”œâ”€â”€ data_ingestion/          # Extract and flatten chat data
â”‚   â”œâ”€â”€ extract_and_flatten.py
â”‚   â”œâ”€â”€ data_lake_storage.py # Data lake storage system
â”‚   â””â”€â”€ chatgpt_url_mapper.py # ChatGPT URL mapping
â”œâ”€â”€ embedding/               # Semantic embedding and clustering
â”‚   â””â”€â”€ embed_and_cluster.py
â”œâ”€â”€ semantic_chunker/        # GPT-driven semantic chunking
â”‚   â”œâ”€â”€ chunker.py          # Main chunking logic
â”‚   â”œâ”€â”€ prompts.py          # Prompt templates
â”‚   â”œâ”€â”€ run_chunking.py     # Script entry point
â”‚   â””â”€â”€ test_sample.json    # Sample for testing
â”œâ”€â”€ neo4j_loader/           # Load data into Neo4j graph
â”‚   â””â”€â”€ load_graph.py
â”œâ”€â”€ api/                    # FastAPI backend
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ frontend/               # React + Cytoscape visualization
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx        # Main visualization component
â”‚   â”‚   â””â”€â”€ index.tsx      # React entry point
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ data/                   # Data storage
â”‚   â”œâ”€â”€ raw/               # ChatGPT export ZIP files
â”‚   â”œâ”€â”€ processed/         # Flattened JSONL data
â”‚   â”œâ”€â”€ embeddings/        # Embeddings and clusters
â”‚   â””â”€â”€ lake/              # Data lake storage
â”‚       â”œâ”€â”€ chats/         # Individual chat JSON files
â”‚       â”œâ”€â”€ messages/      # Individual message JSON files
â”‚       â””â”€â”€ metadata/      # Indexes and metadata
â”œâ”€â”€ run_pipeline.py        # Complete pipeline runner
â”œâ”€â”€ start_services.py      # Service starter
â””â”€â”€ requirements.txt       # Python dependencies
``` 